dataset_file_name,dataset_name,label_name_definition,source,language,text
olid-training-v1.0.tsv,OLID19,"{""subtask_a"": ""Level A: Offensive language identification \n- (NOT) Not Offensive - This post does not contain offense or profanity. \n- (OFF) Offensive - This post contains offensive language or a targeted (veiled or direct) offense \n In our annotation, we label a post as offensive (OFF) if it contains any form of non-acceptable language (profanity) or a targeted offense, which can be veiled or direct. "", ""subtask_b"": ""Level B: Automatic categorization of offense types - (TIN) Targeted Insult and Threats - A post containing an insult or threat to an individual, a group, or others (see categories in sub-task C). \n- (UNT) Untargeted - A post containing non-targeted profanity and swearing. \n Posts containing general profanity are not targeted, but they contain non-acceptable language."", ""subtask_c"":""Level C: Offense target identification \n- (IND) Individual - The target of the offensive post is an individual: a famous person, a named individual or an unnamed person interacting in the conversation. \n - (GRP) Group - The target of the offensive post is a group of people considered as a unity due to the same ethnicity, gender or sexual orientation, political affiliation, religious belief, or something else.\n - (OTH) Other ? ? The target of the offensive post does not belong to any of the previous two categories (e.g., an organization, a situation, an event, or an issue)""}",@Twitter,@eng,tweet
SBFv2.dev.csv;SBFv2.trn.csv;SBFv2.tst.csv,SBFv2,"{""intentYN"":"""", ""sexYN"":"""", ""sexReason"":"""", ""offensiveYN"":""""}",@Twitter,@eng,post
Dynamically Generated Hate Dataset v0.2.3.csv,DGHD,"{""label"": ""a binary variable, indicating whether or not the content has been identified as hateful. It takes two values: hate, nothate."",  
""type"":""a categorical variable, providing a secondary label for hateful content. For hate it can take five values: Animosity, Derogation, Dehumanization, Threatening and Support for Hateful Entities. Please see the paper for more detail. For nothate the 'type' is 'none'. In round 1 the 'type' was not given and is marked as 'notgiven'""}",@None,@eng,text
measuring-hate-speech.csv,MHS,"{""hate_speech_score"": ""Continuous hate speech measure, where higher = more hateful and lower = less hateful. > 0.5 is approximately hate speech, < -1 is counter or supportive speech, and -1 to +0.5 is neutral or ambiguous ""} ",@None,@eng,text
GrimmingerKlingerWASSA2021_train.tsv;GrimmingerKlingerWASSA2021_test.tsv ,USElection2020,{"HOF": "if tweet text is hateful and offensive(Hateful) or neither hateful nor offensive(NonHateful)"},@Twitter,@eng,text
ghc_train.tsv;ghc_test.tsv,GHC,"{""hd"": ""assaults on human dignity"",""cv"": ""Calls for violence"",""vo"": ""Vulgarity and/or Offensive language""} ",@None,@eng,text
hs_AsianPrejudice_20kdataset_cleaned_anonymized.tsv,AsianPrejudice ,"{""hostile.threatening"":"""",""hostile.interpersonal"":"""",""COVID relevant"":"""",""EA relevant"":"""",""hashtags.decision"":"""",""East Asia"":"""",""China"":"""",""Hong Kong"":"""",""Japan"":"""",""Korea"":"""",""Singapore"":"""",""Taiwan"":""""}",@Twitter,@eng,text
Ethos_Dataset_Binary.csv,Ethos,{"isHate":""},@None,@eng,comment
edos_labelled_aggregated.csv,EDOSÂ ,"{""label_sexist"":""TASK A - Binary Sexism Detection: a two-class (or binary) classification where systems have to predict whether a post is sexist or not sexist."",""label_category"":""TASK B - Category of Sexism: for posts which are sexist, a four-class classification where systems have to predict one of four categories: (1) threats, (2)  derogation, (3) animosity, (4) prejudiced discussion."",""label_vector"":""TASK C - Fine-grained Vector of Sexism: for posts which are sexist, an 11-class classification where systems have to predict one of 11 fine-grained vectors."" }","@Gab, @Reddit ",@eng,text
